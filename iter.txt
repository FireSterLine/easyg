
Learning rate equal to 0.05 is in general not good, but it seems not to perform bad when coupled with a higher n_splits and batch size
Learning rate equal to 0.1 not good
5, 7 neurons do not allow any learning

cut -f 2-3,5-10 data.txt | sort -n | grep 0\.05 | sort -n
cut -f 2-3,5-10 data.txt | sort -n | grep 0\.05 | sort -n


cut -f 2-3,5-10 data.txt | sort -n | awk -F "\t" '{ if(($3 != 5) && ($3 != 7)) { print } }'


cat data.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -n | awk -F "\t" '{ if(($3 != 5) && ($3 != 7)) { print } }' | sort -n -k4

cat data.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -n | awk -F "\t" '{ if(($3 != 5) && ($3 != 7)) { print } }' | sort -k4g

n_splits=2 sembra un valore troppo basso

cat data-v2.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -n | awk -F "\t" '{ if(($5 != 2) && ($3 != 7)) { print } }' | sort -k4g

Anche n_splits=5 sembra troppo basso, mentre n_splits=10 sembra meglio. Provo valori più alti
Anche patiente=2 sembra basso. provo più alti

cat data-v2.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -g | awk -F "\t" '{ if(($5 != 2) && ($5 != 2) && ($6 != 2)) { print } }' | sort -k1g

cat data.txt data-v2.txt data-v3.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -g | awk -F "\t" '{ if(($3 > 7) && ($5 != 2) && ($5 != 5) && ($6 != 2)) { print } }'



cat data-v3.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -g
cat data-v4.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -g | awk -F "\t" '{ if(($6 == 7) && ($5 != 2) && ($5 != 5) && ($6 != 2)) { print } }'


cat data*.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -g

Forse? Con meno neuroni (35), learning rate 0.05 è troppo alto
Se il learning rage è alto 0.05, meglio n_splits più alto (35) e batch size più alta (20)
cat data-v4.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5-10 | sort -g | sort -k3g

Try dropout & second layer

cat data-v7.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5- | sort -g | sort -k1g
cat data-v7.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5- | awk -F "\t" '{ if(($1 >= 98.5) && ($2 <= .7)) { print } }' |  sort -g | sort -k3g

try recurrent dropout

Found two spikes.
	for a in $(seq 1 20); do python rnn-cv-rnd.py shifted 35 14 0.2 0.1 0.01 14 10 20 >> spike.tmp; done
	avg: 98.79 +/ 1.09

	for a in $(seq 1 20); do python rnn-cv-rnd.py shifted 140 5 0.01 0.5 0.005 14 10 20 >> spike-2.tmp; done
	avg: 98.86 +/- 0.94

cat data-v9.txt | awk -F "\t" '{ if(($4 ~ /\|/)) { print } }' | cut -f 2-3,5- | awk -F "\t" '{ if(($1 >= 98.5) && ($2 <= .7)) { print } }' |  sort -g | sort -k{3..8}g

Increase epocs
Early stopping: val_loss instead of val_acc

grep _ data-v10-loss+60ep.txt

patience = 12 is the best
